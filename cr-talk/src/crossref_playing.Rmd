

Crossref TDM full text link coverage
=======

Crossref has a Text and Data Mining http://tdmsupport.crossref.org/ initiative. Basically, it boils down to publishers including links to full text versions of their articles meant for text mining (typically xml or pdf).  

We use this in our `fulltext` package for text mining to fetch full text data for articles of interest ->  https://github.com/ropensci/fulltext

One question of interest to me, and I imagine others, is how are publishers doing WRT including links for full text?  The Crossref search API has a field called `coverage` (e.g., look at http://api.crossref.org/members/12) that has details on coverage for full text links included in their metadata given to Crossref, among other things. 

We have an R client, `rcrossref`, that wraps the Crossref search API, so let's do this thing. 


### setup

Install


```{r}
install.packages(c("dplyr", "ggplot2", "rcrossref", "httr", "knitr"))
devtools::install_github("ropensci/rcrossref")
```

Load

```{r}
library("rcrossref")
library("dplyr")
library("ggplot2")
library("httr")
```

### Get member IDs

Get total number of members


```{r}
tot <- cr_members(limit = 1)
tot$meta$total_results
#> [1] 5450
```


```{r}
res <- lapply(seq(1, tot$meta$total_results, 500), function(x) {
  cr_members(limit = 500, offset = x)
})
(res_df <- rbind_all(lapply(res, "[[", "data")))
```

### Get coverage data

```{r}
df <- res_df %>% 
  select(id, primary_name, total.dois, coverage = coverge.resource.links.current)
df$coverage[grep("e", df$coverage)] <- 
  sapply(df$coverage[grep("e", df$coverage)], function(z) {
  sub("_", "-", z)
}, USE.NAMES = FALSE)
df$coverage <- as.numeric(df$coverage)
df$total.dois <- as.numeric(df$total.dois)
```

### Plot

> wowsers, lots of publishers have no data

```{r}
ggplot(df, aes(x = coverage)) +
  geom_histogram(binwidth = 0.01) +
  theme_grey(base_size = 18)
```

![plot of chunk unnamed-chunk-8](http://i.imgur.com/poxZGDr.png) 

Remove zeros

> the publishsers with non-zero coverage values


```{r}
dfnozeros <- df %>% filter(coverage > 0)
ggplot(dfnozeros, aes(x = coverage)) +
  geom_histogram(binwidth = 0.03) +
  theme_grey(base_size = 18)
```

![plot of chunk unnamed-chunk-9](http://i.imgur.com/XBHIArr.png) 

Who's doing the best? Well, 142 have coverage of 100%, so let's pick the ones with high coverage and the most DOIs:

```{r}
df_high <- dfnozeros %>% 
  filter(coverage == 1) %>% 
  arrange(desc(total.dois))
knitr::kable(df_high[1:10,])
```

What about some of the big, well-known publishers + cool open access publishers (subjective, I know)?

```{r}
pubids <- c(78, 197, 311, 340, 4374, 4443, 263, 286, 179, 316, 301, 1121, 276, 56)
res <- df %>% 
  filter(id %in% pubids) %>% 
  arrange(desc(coverage))
knitr::kable(res)
```

We have some work to do, eh?

## How many actually resolve? Sampling to avoid hitting servers too hard

Remove those that have 0 coverage

```{r}
ids <- df %>% 
  filter(id %in% pubids, coverage > 0) %>% 
  select(id) %>% .$id
```

Ping each link

```{r}
data <- lapply(ids, function(x) {
  # Get works data for 500
  res <- cr_members(member_ids = x, works = TRUE, 
                    filter = c(has_full_text = TRUE), limit = 50)
  # Pull out links (they each only have 1)
  lks <- res$data$link_link1_URL
  # Try to make a GET request to each URL. Return `FALSE` if failed, `TRUE` if success
  vapply(lks, function(z) {
    tmp <- tryCatch(httr::GET(z), error = function(e) e)
    if (is(tmp, "error")) FALSE else TRUE
  }, logical(1))
})
data <- setNames(data, ids)
df <- setNames(plyr::ldply(data, function(z) {
  (length(z[z]) / 50) * 100
}), c("publisher", "no_resolve"))
```

Make a table of results, show first 6 rows

```{r}
knitr::kable(df, format = "html")
```
